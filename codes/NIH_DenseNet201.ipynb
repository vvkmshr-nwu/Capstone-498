{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NIH Chest X-ray Dataset Explanation\n",
    "\n",
    "## Introduction\n",
    "The **NIH Chest X-ray Dataset** is a large-scale dataset provided by the National Institutes of Health (NIH), containing **112,120 frontal-view chest X-ray images** from **30,805 unique patients**. The dataset was introduced to support the development of deep learning models for automated diagnosis of thoracic diseases.\n",
    "\n",
    "## Key Features\n",
    "- **Large-scale dataset**: Contains over 112,000 X-ray images.\n",
    "- **Multi-label classification**: Includes annotations for **14 different thoracic diseases**, such as pneumonia, edema, emphysema, and fibrosis.\n",
    "- **Metadata Availability**: Includes patient age, gender, view position, and image index.\n",
    "- **Open-Source**: Publicly available for research and development in medical AI applications.\n",
    "\n",
    "## Labels and Diseases\n",
    "The dataset provides labels for the following 14 thoracic conditions:\n",
    "- Atelectasis\n",
    "- Cardiomegaly\n",
    "- Consolidation\n",
    "- Edema\n",
    "- Effusion\n",
    "- Emphysema\n",
    "- Fibrosis\n",
    "- Hernia\n",
    "- Infiltration\n",
    "- Mass\n",
    "- Nodule\n",
    "- Pleural Thickening\n",
    "- Pneumonia\n",
    "- Pneumothorax\n",
    "\n",
    "```\n",
    "\n",
    "## Applications\n",
    "- **Automated disease diagnosis** using deep learning models.\n",
    "- **Medical AI research** for improving diagnostic accuracy.\n",
    "- **Explainability and interpretability studies** to analyze model decision-making.\n",
    "\n",
    "The NIH Chest X-ray Dataset has been widely used in medical imaging research, contributing to advancements in AI-powered diagnostics.\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DenseNet201 Model Explanation\n",
    "\n",
    "## Introduction\n",
    "DenseNet201 is a deep convolutional neural network that belongs to the DenseNet family, which stands for **Densely Connected Convolutional Networks**. It was introduced in the paper *\"Densely Connected Convolutional Networks\"* by Gao Huang et al. in 2017.\n",
    "\n",
    "## Key Features\n",
    "- **Dense Connectivity**: Each layer receives input from all preceding layers and passes its feature maps to all subsequent layers.\n",
    "- **Efficient Parameter Utilization**: Due to the dense connections, the network requires fewer parameters compared to traditional architectures.\n",
    "- **Alleviation of Vanishing Gradient Problem**: By allowing shorter connections between layers close to the input and those close to the output, DenseNet helps in better gradient flow.\n",
    "- **Feature Reuse**: Enhances learning efficiency by enabling the reuse of previously learned features.\n",
    "\n",
    "## Architecture\n",
    "DenseNet201 consists of:\n",
    "- **Convolutional Layer**: Initial feature extraction.\n",
    "- **Dense Blocks**: Several layers with Batch Normalization, ReLU, and Convolution, densely connected.\n",
    "- **Transition Layers**: Reduces dimensionality using 1x1 convolution and pooling layers.\n",
    "- **Global Average Pooling**: Reduces spatial dimensions before fully connected layers.\n",
    "- **Fully Connected Layer**: Final classification output.\n",
    "\n",
    "## Model Usage in TensorFlow/Keras\n",
    "```python\n",
    "from tensorflow.keras.applications import DenseNet201\n",
    "\n",
    "# Load pre-trained DenseNet201 model\n",
    "model = DenseNet201(weights='imagenet', include_top=True)\n",
    "model.summary()\n",
    "```\n",
    "\n",
    "DenseNet201 is widely used in image classification tasks and can be fine-tuned for various applications, including medical imaging and object detection.\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 0.776,
     "end_time": "2021-02-08T01:26:41.641342",
     "exception": false,
     "start_time": "2021-02-08T01:26:40.865342",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ML tools \n",
    "import tensorflow as tf\n",
    "# from kaggle_datasets import KaggleDatasets\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.optimizers import Adam\n",
    "from tensorflow.keras import layers, Model, optimizers\n",
    "# import tensorflow.keras.applications.efficientnet as efn\n",
    "from tensorflow.keras.applications import *\n",
    "import os\n",
    "# from tensorflow.keras.applications import DenseNet201 \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Load and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "papermill": {
     "duration": 0.237485,
     "end_time": "2021-02-08T01:26:41.898073",
     "exception": false,
     "start_time": "2021-02-08T01:26:41.660588",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('./Data_Entry_2017.csv')\n",
    "df.img_ind= df.img_ind.apply(lambda x: x.split('.')[0])\n",
    "\n",
    "\n",
    "# df[\"img_ind\"] = \"NIH_Images\\\\\" + df[\"img_ind\"]\n",
    "\n",
    "# NIH_Images\\00012255_000.jpg\n",
    "\n",
    "display(df.head(4))\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"./NIH_images\"  # Change to your target directory\n",
    "files = os.listdir(directory)  # List all files in the directory\n",
    "files = sorted(files)  # Optional: Sort files alphabetically\n",
    "\n",
    "print(\"Top 5 files:\", files[:5])  # Print first 5 files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.027297,
     "end_time": "2021-02-08T01:26:42.266735",
     "exception": false,
     "start_time": "2021-02-08T01:26:42.239438",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "target_cols = df.drop(['img_ind'], axis=1).columns.to_list()\n",
    "n_classes = len(target_cols)\n",
    "img_size = 600\n",
    "n_epochs = 35\n",
    "lr= 0.0001\n",
    "seed= 11\n",
    "val_split= 0.2\n",
    "seed= 33\n",
    "batch_size=12\n",
    "n_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation for faster ingestion during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.041826,
     "end_time": "2021-02-08T01:26:42.328543",
     "exception": false,
     "start_time": "2021-02-08T01:26:42.286717",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def auto_select_accelerator():\n",
    "    try:\n",
    "        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "        tf.config.experimental_connect_to_cluster(tpu)\n",
    "        tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "        strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "        print(\"Running on TPU:\", tpu.master())\n",
    "    except ValueError:\n",
    "        strategy = tf.distribute.get_strategy()\n",
    "    print(f\"Running on {strategy.num_replicas_in_sync} replicas\")\n",
    "    \n",
    "    return strategy\n",
    "\n",
    "'''\n",
    "Reference\n",
    "https://www.kaggle.com/xhlulu/ranzcr-efficientnet-tpu-training\n",
    "\n",
    "'''\n",
    "\n",
    "def build_decoder(with_labels=True, target_size=(img_size, img_size), ext='jpg'):\n",
    "    def decode(path):\n",
    "        file_bytes = tf.io.read_file(path) # Reads and outputs the entire contents of the input filename.\n",
    "\n",
    "        if ext == 'png':\n",
    "            img = tf.image.decode_png(file_bytes, channels=3) # Decode a PNG-encoded image to a uint8 or uint16 tensor\n",
    "        elif ext in ['jpg', 'jpeg']:\n",
    "            img = tf.image.decode_jpeg(file_bytes, channels=3) # Decode a JPEG-encoded image to a uint8 tensor\n",
    "        else:\n",
    "            raise ValueError(\"Image extension not supported\")\n",
    "\n",
    "        img = tf.cast(img, tf.float32) / 255.0 # Casts a tensor to the type float32 and divides by 255.\n",
    "        img = tf.image.resize(img, target_size) # Resizing to target size\n",
    "        return img\n",
    "    \n",
    "    def decode_with_labels(path, label):\n",
    "        return decode(path), label\n",
    "    \n",
    "    return decode_with_labels if with_labels else decode\n",
    "\n",
    "\n",
    "def build_augmenter(with_labels=True):\n",
    "    def augment(img):\n",
    "        img = tf.image.random_flip_left_right(img)\n",
    "        img = tf.image.random_flip_up_down(img)\n",
    "        img = tf.image.random_saturation(img, 0.8, 1.2)\n",
    "        img = tf.image.random_brightness(img, 0.1)\n",
    "        img = tf.image.random_contrast(img, 0.8, 1.2)\n",
    "        return img\n",
    "    \n",
    "    def augment_with_labels(img, label):\n",
    "        return augment(img), label\n",
    "    \n",
    "    return augment_with_labels if with_labels else augment\n",
    "\n",
    "def build_dataset(paths, labels=None, bsize=32, cache=True,\n",
    "                  decode_fn=None, augment_fn=None,\n",
    "                  augment=True, repeat=True, shuffle=1024, \n",
    "                  cache_dir=\"\"):\n",
    "    if cache_dir != \"\" and cache is True:\n",
    "        os.makedirs(cache_dir, exist_ok=True)\n",
    "    \n",
    "    if decode_fn is None:\n",
    "        decode_fn = build_decoder(labels is not None)\n",
    "    \n",
    "    if augment_fn is None:\n",
    "        augment_fn = build_augmenter(labels is not None)\n",
    "    \n",
    "    AUTO = tf.data.experimental.AUTOTUNE\n",
    "    slices = paths if labels is None else (paths, labels)\n",
    "    \n",
    "    dset = tf.data.Dataset.from_tensor_slices(slices)\n",
    "    dset = dset.map(decode_fn, num_parallel_calls=AUTO)\n",
    "    dset = dset.cache(cache_dir) if cache else dset\n",
    "    dset = dset.map(augment_fn, num_parallel_calls=AUTO) if augment else dset\n",
    "    dset = dset.repeat() if repeat else dset\n",
    "    dset = dset.shuffle(shuffle) if shuffle else dset\n",
    "    dset = dset.batch(bsize).prefetch(AUTO) # overlaps data preprocessing and model execution while training\n",
    "    return dset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 6.314494,
     "end_time": "2021-02-08T01:26:48.663271",
     "exception": false,
     "start_time": "2021-02-08T01:26:42.348777",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "strategy = auto_select_accelerator()\n",
    "batch_size = strategy.num_replicas_in_sync * batch_size\n",
    "print('batch size', batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.11783,
     "end_time": "2021-02-08T01:26:48.801455",
     "exception": false,
     "start_time": "2021-02-08T01:26:48.683625",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "paths = \"./NIH_images/\" + df['img_ind'] + '.jpg'\n",
    "\n",
    "print(paths)\n",
    "#Get the multi-labels\n",
    "label_cols = df.columns[:-1]\n",
    "labels = df[label_cols].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.056789,
     "end_time": "2021-02-08T01:26:48.878703",
     "exception": false,
     "start_time": "2021-02-08T01:26:48.821914",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "(train_paths, valid_paths, \n",
    "  train_labels, valid_labels) = train_test_split(paths, labels, test_size=val_split, random_state=11)\n",
    "\n",
    "print(train_paths.shape, valid_paths.shape)\n",
    "train_labels.sum(axis=0), valid_labels.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.372349,
     "end_time": "2021-02-08T01:26:49.431926",
     "exception": false,
     "start_time": "2021-02-08T01:26:49.059577",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Build the tensorflow datasets\n",
    "\n",
    "decoder = build_decoder(with_labels=True, target_size=(img_size, img_size))\n",
    "\n",
    "# Build the tensorflow datasets\n",
    "dtrain = build_dataset(\n",
    "    train_paths, train_labels, bsize=batch_size, decode_fn=decoder\n",
    ")\n",
    "\n",
    "dvalid = build_dataset(\n",
    "    valid_paths, valid_labels, bsize=batch_size, \n",
    "    repeat=False, shuffle=False, augment=False, decode_fn=decoder\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, _ = dtrain.take(2)\n",
    "images = data[0].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 4, figsize=(20,10))\n",
    "axes = axes.flatten()\n",
    "for img, ax in zip(images, axes):\n",
    "    ax.imshow(img)\n",
    "    ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.033245,
     "end_time": "2021-02-08T01:26:49.48767",
     "exception": false,
     "start_time": "2021-02-08T01:26:49.454425",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the Model Using DenseNet201\n",
    "def build_model():\n",
    "    base = DenseNet201(input_shape=(img_size, img_size, 3), include_top=False, weights='imagenet')\n",
    "\n",
    "    inp = layers.Input(shape=(img_size, img_size, 3))\n",
    "    x = base(inp)\n",
    "    x = layers.GlobalAveragePooling2D()(layers.Dropout(0.16)(x))\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    x = layers.Dense(n_classes, activation='sigmoid')(x)  # Multi-label classification\n",
    "\n",
    "    return Model(inp, x)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 122.352746,
     "end_time": "2021-02-08T01:28:51.862611",
     "exception": false,
     "start_time": "2021-02-08T01:26:49.509865",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    model= build_model()\n",
    "    loss= tf.keras.losses.BinaryCrossentropy(label_smoothing=0.0)\n",
    "    model.compile(optimizers.Adam(learning_rate=lr),loss=loss,metrics=[tf.keras.metrics.AUC(multi_label=True)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.060939,
     "end_time": "2021-02-08T01:28:52.216709",
     "exception": false,
     "start_time": "2021-02-08T01:28:52.15577",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# name= 'NIH_Seresnet152_model.h5'\n",
    "\n",
    "# rlr = ReduceLROnPlateau(monitor = 'val_loss', factor = 0.1, patience = 2, verbose = 1, \n",
    "#                                 min_delta = 1e-4, min_lr = 1e-6, mode = 'min', cooldown=1)\n",
    "        \n",
    "# ckp = ModelCheckpoint(name,monitor = 'val_loss',\n",
    "#                       verbose = 1, save_best_only = True, mode = 'min')\n",
    "        \n",
    "# es = EarlyStopping(monitor = 'val_loss', min_delta = 1e-4, patience = 5, mode = 'min', \n",
    "#                     restore_best_weights = True, verbose = 1)\n",
    "\n",
    "name = \"NIH_DenseNet201_model.h5\"\n",
    "\n",
    "rlr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=2, verbose=1, \n",
    "                        min_delta=1e-4, min_lr=1e-6, mode='min', cooldown=1)\n",
    "\n",
    "ckp = ModelCheckpoint(name, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', min_delta=1e-4, patience=5, mode='min', \n",
    "                   restore_best_weights=True, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.054513,
     "end_time": "2021-02-08T01:28:52.313072",
     "exception": false,
     "start_time": "2021-02-08T01:28:52.258559",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "steps_per_epoch = (train_paths.shape[0] // batch_size)\n",
    "steps_per_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 4545.439448,
     "end_time": "2021-02-08T02:44:37.790427",
     "exception": false,
     "start_time": "2021-02-08T01:28:52.350979",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "history = model.fit(dtrain,                      \n",
    "                    validation_data=dvalid,                                       \n",
    "                    epochs=n_epochs,\n",
    "                    callbacks=[rlr,es,ckp],\n",
    "                    steps_per_epoch=steps_per_epoch,\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"workspace/NIH_DenseNet201_model.h5\"\n",
    "\n",
    "model.save('./NIH_DenseNet201_model.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.74803,
     "end_time": "2021-02-08T02:44:39.033205",
     "exception": false,
     "start_time": "2021-02-08T02:44:38.285175",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12, 6))\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.plot( history.history[\"loss\"], label = \"Training Loss\", marker='o')\n",
    "plt.plot( history.history[\"val_loss\"], label = \"Validation Loss\", marker='+')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.693171,
     "end_time": "2021-02-08T02:44:40.279118",
     "exception": false,
     "start_time": "2021-02-08T02:44:39.585947",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12, 6))\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"AUC\")\n",
    "plt.plot( history.history[\"auc\"], label = \"Training AUC\" , marker='o')\n",
    "plt.plot( history.history[\"val_auc\"], label = \"Validation AUC\", marker='+')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 501.673242,
     "end_time": "2021-02-08T02:53:02.443459",
     "exception": false,
     "start_time": "2021-02-08T02:44:40.770217",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "model= tf.keras.models.load_model(name,  safe_mode=False)\n",
    "pred= model.predict(dvalid, verbose=1)\n",
    "\n",
    "print('AUC CKECK-UP per CLASS')\n",
    "\n",
    "classes= df.columns[:-1]\n",
    "for i, n in enumerate(classes):\n",
    "  print(classes[i])\n",
    "  print(i, roc_auc_score(valid_labels[:, i], pred[:, i]))\n",
    "  print('---------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear session to free memory\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# Load the trained model\n",
    "model = tf.keras.models.load_model(name, safe_mode=False)\n",
    "\n",
    "# Get model predictions\n",
    "pred = model.predict(dvalid, verbose=1)\n",
    "\n",
    "# Define disease classes\n",
    "classes = df.columns[:-1]\n",
    "\n",
    "# Compute AUC scores for each class\n",
    "auc_scores = []\n",
    "for i, n in enumerate(classes):\n",
    "    auc = roc_auc_score(valid_labels[:, i], pred[:, i])\n",
    "    auc_scores.append(auc)\n",
    "\n",
    "# Convert to NumPy array for easy sorting (optional)\n",
    "auc_scores = np.array(auc_scores)\n",
    "\n",
    "# **Plot the AUC scores**\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.barh(classes, auc_scores, color='royalblue')\n",
    "plt.xlabel(\"AUC Score\")\n",
    "plt.ylabel(\"Disease Classes\")\n",
    "plt.title(\"AUC Score per Disease Class\")\n",
    "plt.xlim(0, 1)  # AUC scores range from 0 to 1\n",
    "plt.grid(axis=\"x\", linestyle=\"--\", alpha=0.5)\n",
    "\n",
    "# Annotate bars with values\n",
    "for index, value in enumerate(auc_scores):\n",
    "    plt.text(value + 0.02, index, f\"{value:.3f}\", va=\"center\", fontsize=10)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# Clear TensorFlow session\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# Load the trained model\n",
    "model = tf.keras.models.load_model(name, safe_mode=False)\n",
    "\n",
    "# Get model predictions\n",
    "pred = model.predict(dvalid, verbose=1)\n",
    "\n",
    "# Define disease classes\n",
    "classes = df.columns[:-1]\n",
    "\n",
    "# Initialize figure for multiple ROC curves\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Loop through each disease class and plot its ROC curve\n",
    "for i, disease in enumerate(classes):\n",
    "    fpr, tpr, _ = roc_curve(valid_labels[:, i], pred[:, i])  # Compute FPR, TPR\n",
    "    roc_auc = auc(fpr, tpr)  # Compute AUC\n",
    "    plt.plot(fpr, tpr, label=f\"{disease} (AUC = {roc_auc:.3f})\")  # Plot each ROC curve\n",
    "\n",
    "# Plot diagonal line (random classifier)\n",
    "plt.plot([0, 1], [0, 1], 'k--', label=\"Random (AUC = 0.5)\")\n",
    "\n",
    "# Set axis labels and title\n",
    "plt.xlabel(\"False Positive Rate (FPR)\")\n",
    "plt.ylabel(\"True Positive Rate (TPR)\")\n",
    "plt.title(\"ROC Curves for All Disease Classes\")\n",
    "plt.legend(loc=\"lower right\")  # Add legend\n",
    "plt.grid(alpha=0.3)  # Light grid for better readability\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Explainability: Gradcam visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "def create_gradcam_model(model):\n",
    "    \"\"\"\n",
    "    Creates a model for Grad-CAM, identifying the base feature extractor \n",
    "    and the last convolutional layer.\n",
    "    \"\"\"\n",
    "    # Print model summary to understand its structure\n",
    "    print(\"Model structure:\")\n",
    "    model.summary(line_length=100)\n",
    "    \n",
    "    # First, find the feature extractor (DenseNet) and target dense layer\n",
    "    feature_extractor = None\n",
    "    dense_layer = None\n",
    "    \n",
    "    for layer in model.layers:\n",
    "        if 'densenet' in layer.name:\n",
    "            feature_extractor = layer\n",
    "        if isinstance(layer, tf.keras.layers.Dense) and layer.name == 'dense':\n",
    "            dense_layer = layer\n",
    "    \n",
    "    if feature_extractor is None:\n",
    "        raise ValueError(\"Could not find DenseNet feature extractor in the model\")\n",
    "    if dense_layer is None:\n",
    "        raise ValueError(\"Could not find final dense layer in the model\")\n",
    "    \n",
    "    # Now, create a direct connection between input and feature extractor's output\n",
    "    # This is a cleaner approach than trying to search for specific internal layers\n",
    "    \n",
    "    # Get the input\n",
    "    model_input = model.input\n",
    "    \n",
    "    # Get the output of the feature extractor (before global pooling)\n",
    "    # For DenseNet, this would be the feature maps before global pooling\n",
    "    feature_maps = feature_extractor.output\n",
    "    \n",
    "    # Get the model's output (classification)\n",
    "    predictions = model.output\n",
    "    \n",
    "    # Create a model that outputs both the feature maps and the predictions\n",
    "    gradcam_model = Model(inputs=model_input, outputs=[feature_maps, predictions])\n",
    "    \n",
    "    return gradcam_model\n",
    "\n",
    "def generate_gradcam(model, img_array, class_idx):\n",
    "    \"\"\"\n",
    "    Generates a Grad-CAM heatmap using a direct approach.\n",
    "    \"\"\"\n",
    "    # Create the Grad-CAM model\n",
    "    gradcam_model = create_gradcam_model(model)\n",
    "    \n",
    "    # Use gradient tape to compute gradients\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Cast the image to float32 to ensure proper gradient computation\n",
    "        img_array = tf.cast(img_array, tf.float32)\n",
    "        \n",
    "        # Watch the input image\n",
    "        tape.watch(img_array)\n",
    "        \n",
    "        # Get feature maps and predictions\n",
    "        feature_maps, predictions = gradcam_model(img_array)\n",
    "        \n",
    "        # Get the prediction for the target class\n",
    "        target_class_prediction = predictions[:, class_idx]\n",
    "    \n",
    "    # Calculate gradients of the target class prediction with respect to feature maps\n",
    "    grads = tape.gradient(target_class_prediction, feature_maps)\n",
    "    \n",
    "    # Global average pooling of gradients\n",
    "    pooled_grads = tf.reduce_mean(grads, axis=(1, 2))\n",
    "    \n",
    "    # Multiply feature maps with weighted gradients\n",
    "    # This highlights the regions that contribute most to the target class\n",
    "    feature_maps = feature_maps[0]\n",
    "    pooled_grads = pooled_grads[0]\n",
    "    \n",
    "    # Apply gradient weights to each channel of the feature map\n",
    "    heatmap = tf.zeros_like(feature_maps[:, :, 0])\n",
    "    \n",
    "    for i in range(pooled_grads.shape[0]):\n",
    "        heatmap += feature_maps[:, :, i] * pooled_grads[i]\n",
    "    \n",
    "    # Apply ReLU to focus on positive contributions\n",
    "    heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)\n",
    "    \n",
    "    # Convert to numpy array\n",
    "    heatmap = heatmap.numpy()\n",
    "    \n",
    "    return heatmap\n",
    "\n",
    "def overlay_gradcam(img_path, heatmap, alpha=0.5):\n",
    "    \"\"\"\n",
    "    Overlays Grad-CAM heatmap on the original image.\n",
    "    Args:\n",
    "    - img_path: Path to the original image\n",
    "    - heatmap: Grad-CAM heatmap\n",
    "    - alpha: Transparency factor\n",
    "    Returns:\n",
    "    - superimposed_img: Image with heatmap overlay\n",
    "    \"\"\"\n",
    "    img = cv2.imread(img_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Resize heatmap to match original image size\n",
    "    heatmap = cv2.resize(heatmap, (img.shape[1], img.shape[0]))\n",
    "    \n",
    "    # Convert heatmap to RGB format\n",
    "    heatmap = np.uint8(255 * heatmap)\n",
    "    heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n",
    "    \n",
    "    # Superimpose heatmap on the original image\n",
    "    superimposed_img = cv2.addWeighted(heatmap, alpha, img, 1 - alpha, 0)\n",
    "    \n",
    "    return superimposed_img\n",
    "\n",
    "# Main execution\n",
    "# ðŸ”¹ Load your trained model\n",
    "model = tf.keras.models.load_model(name)\n",
    "\n",
    "# ðŸ”¹ Select the image and class to analyze\n",
    "img_path = \"./NIH_images/00030786_007.jpg\"\n",
    "img_size = 600  # Adjust based on your model's input size\n",
    "class_idx = 2  # Example: Cardiomegaly\n",
    "\n",
    "# ðŸ”¹ Preprocess the image\n",
    "img = tf.keras.preprocessing.image.load_img(img_path, target_size=(img_size, img_size))\n",
    "img_array = tf.keras.preprocessing.image.img_to_array(img)\n",
    "img_array = np.expand_dims(img_array, axis=0)\n",
    "img_array /= 255.0  # Normalize\n",
    "\n",
    "# ðŸ”¹ Generate Grad-CAM heatmap\n",
    "try:\n",
    "    heatmap = generate_gradcam(model, img_array, class_idx)\n",
    "    \n",
    "    # ðŸ”¹ Overlay Grad-CAM on the original image\n",
    "    superimposed_img = overlay_gradcam(img_path, heatmap)\n",
    "    \n",
    "    # ðŸ”¹ Plot the results\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    ax[0].imshow(img)\n",
    "    ax[0].set_title(\"Original X-ray\")\n",
    "    ax[0].axis(\"off\")\n",
    "    ax[1].imshow(superimposed_img)\n",
    "    ax[1].set_title(\"Grad-CAM Heatmap\")\n",
    "    ax[1].axis(\"off\")\n",
    "    plt.show()\n",
    "except Exception as e:\n",
    "    print(f\"Error: {str(e)}\")\n",
    "    \n",
    "    # Fallback to an even simpler approach if needed\n",
    "    print(\"Attempting fallback approach...\")\n",
    "    \n",
    "    # This is a last resort approach if everything else fails\n",
    "    # It uses a simpler method that doesn't rely on internal model structure\n",
    "    \n",
    "    def simple_gradcam(model, img_array, class_idx):\n",
    "        \"\"\"\n",
    "        A simplified Grad-CAM implementation that works with any model structure.\n",
    "        \"\"\"\n",
    "        # Create a model that outputs both the final dense layer's input and the prediction\n",
    "        # This assumes the model's last layers are GlobalAveragePooling followed by Dense\n",
    "        for i, layer in enumerate(model.layers):\n",
    "            if isinstance(layer, tf.keras.layers.Dense) and layer.name == 'dense':\n",
    "                # Find the layer that feeds into this dense layer\n",
    "                prev_layer = model.layers[i-1]\n",
    "                break\n",
    "        else:\n",
    "            raise ValueError(\"Could not find the dense layer\")\n",
    "        \n",
    "        # Create a model that gets the output of the layer before dense\n",
    "        intermediate_model = Model(inputs=model.input, outputs=prev_layer.output)\n",
    "        \n",
    "        # Get the feature map representation\n",
    "        features = intermediate_model.predict(img_array)\n",
    "        \n",
    "        # Get the prediction for the target class\n",
    "        prediction = model.predict(img_array)[0, class_idx]\n",
    "        \n",
    "        # Create a gradient-weighted class activation map\n",
    "        # Since we can't trace gradients easily this way, we'll use the feature importance\n",
    "        # This is not a true Grad-CAM but a simplified visualization\n",
    "        \n",
    "        # Reshape the features to a 2D array and create a basic heatmap\n",
    "        if len(features.shape) > 2:  # If features still have spatial dimensions\n",
    "            heatmap = np.mean(features[0], axis=-1)\n",
    "        else:\n",
    "            # For global pooled features, we create a placeholder heatmap\n",
    "            # This is just for visualization, not a true Grad-CAM\n",
    "            heatmap = np.ones((10, 10))  # Placeholder\n",
    "            \n",
    "        return heatmap\n",
    "    \n",
    "    # Try the fallback approach\n",
    "    heatmap = simple_gradcam(model, img_array, class_idx)\n",
    "    superimposed_img = overlay_gradcam(img_path, heatmap)\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    ax[0].imshow(img)\n",
    "    ax[0].set_title(\"Original X-ray\")\n",
    "    ax[0].axis(\"off\")\n",
    "    ax[1].imshow(superimposed_img)\n",
    "    ax[1].set_title(\"Feature Visualization (Fallback)\")\n",
    "    ax[1].axis(\"off\")\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 5839,
     "sourceId": 18613,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 620959,
     "sourceId": 1108231,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 1145535,
     "sourceId": 1920904,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 1145594,
     "sourceId": 1920987,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30299,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
